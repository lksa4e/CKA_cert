1. Create a new service account with the name pvviewer. 
Grant this Service account access to list all PersistentVolumes in the cluster by creating an appropriate cluster role 
called pvviewer-role and ClusterRoleBinding called pvviewer-role-binding.
Next, create a pod called pvviewer with the image: redis and serviceAccount: pvviewer in the default namespace

ServiceAccount: pvviewer
ClusterRole: pvviewer-role
ClusterRoleBinding: pvviewer-role-binding
Pod: pvviewer
Pod configured to use ServiceAccount pvviewer?

1. 서비스 어카운트 생성 pvviewer
2. Cluster Role pvviewer-role
3. pvviewer-role-binding
4. Pod 생성

kubectl create serviceaccount pvviewer
kubectl create clusterrole pvviewer-role --verb=list --resource=PersistentVolumes
kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer

apiVersion: v1
kind: Pod
metadata:
  name: pvviewer
spec:
  containers:
  - image: redis
    name: pvviewer
  serviceAccountName: pvviewer

==> pod의 describe에서 제대로 되었는지 확인 가능함

2. List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips

Answer should be in the format: InternalIP of master<space>InternalIP of node1<space>InternalIP of 
node2<space>InternalIP of node3 (in a single line)

kubectl get nodes -o=jsonpath='{.items[*].status.addresses[0].address}' > /root/CKA/node_ips

3. Create a pod called multi-pod with two containers.
Container 1, name: alpha, image: nginx
Container 2: beta, image: busybox, command sleep 4800.

Environment Variables:
container 1:
name: alpha

Container 2:
name: beta

apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  containers:
  - name: alpha
    image: nginx
    env:
    - name: name
      value: alpha
  - name: beta
    image: busybox
    env:
    - name: name
      value: beta
    command: ["sleep","4800"]

4. Create a Pod called non-root-pod , image: redis:alpine
runAsUser: 1000
fsGroup: 2000

apiVersion: v1
kind: Pod
metadata:
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  containers:
  - name: non-root-pod
    image: redis:alpine
    securityContext:
      allowPrivilegeEscalation: false


kubectl exec -it non-root-pod -- sh

5. We have deployed a new pod called np-test-1 and a service called np-test-service. 
Incoming connections to this service are not working. Troubleshoot and fix it.
Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections to the service over port 80

svc : run=np=test=1

Important: Don't delete any current objects deployed.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
  - Ingress
  ingress:
  - ports:
    - protocol: TCP
      port: 80

==> network policy 현재 가지고있는거 describe 하면 모든 ingress deny 되어있는거 확인 가능
==> busybox 컨테이너 만들어서 테스트 할 수 있음

6. Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, 
image redis:alpine to ensure workloads are not scheduled to this worker node. 
Finally, create a new pod called prod-redis and image redis:alpine with toleration to be scheduled on node01.

key:env_type, value:production, operator: Equal and effect:NoSchedule

kubectl taint nodes node01 env_type=production:NoSchedule

apiVersion: v1
kind: Pod
metadata:
  name: prod-redis
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "env_type"
    value: "production"
    operator: "Equal"
    effect: "NoSchedule"

7. Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier .
image: redis:alpine

Use appropriate labels and create all the required objects if it does not exist in the system already.


apiVersion: v1
kind: Pod
metadata:
  name: hr-pod
  labels:
    environment: production
    tier: frontend
  namespace: hr
spec:
  containers:
  - name: hr-pod
    image: redis:alpine

8. A kubeconfig file called super.kubeconfig has been created under /root/CKA. 
There is something wrong with the configuration. Troubleshoot and fix it.

kubectl config view

==> kubectl cluster-info --kubeconfig=/root/super.kubeconfig

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://172.17.0.30:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

9. We have created a new deployment called nginx-deploy. 
scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it.
